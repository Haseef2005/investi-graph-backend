import os
import shutil
import glob
from sec_edgar_downloader import Downloader
from bs4 import BeautifulSoup
from app.config import settings
from app import processing, crud, models
from app.database import SessionLocal
import logging
import re
from app.utils import smart_crop_content

log = logging.getLogger("uvicorn.error")

TEMP_SEC_DIR = "/app/temp_sec" # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏û‡∏±‡∏Å‡πÑ‡∏ü‡∏•‡πå

def clean_html_content(raw_content: str) -> str:
    """
    1. Extract only the '10-K' document section from the full submission.
    2. Remove HTML tags.
    3. Clean up whitespace.
    """
    if not raw_content:
        return ""

    # --- Step 1: ‡∏´‡∏≤ Document ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å (10-K, 10-Q, 20-F) ---
    # Pattern: ‡∏´‡∏≤ <DOCUMENT> ‡∏ó‡∏µ‡πà‡∏Ç‡πâ‡∏≤‡∏á‡πÉ‡∏ô‡∏°‡∏µ <TYPE>10-K... ‡πÅ‡∏•‡πâ‡∏ß‡∏î‡∏∂‡∏á <TEXT> ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
    # (?s) ‡∏Ñ‡∏∑‡∏≠‡πÉ‡∏´‡πâ . match newlines ‡πÑ‡∏î‡πâ
    
    # ‡∏•‡∏≠‡∏á‡∏´‡∏≤ 10-K ‡∏´‡∏£‡∏∑‡∏≠ 10-Q ‡∏´‡∏£‡∏∑‡∏≠ 20-F
    doc_match = re.search(
        r'<DOCUMENT>\s*<TYPE>(?:10-K|10-Q|20-F).*?<TEXT>(.*?)</TEXT>\s*</DOCUMENT>', 
        raw_content, 
        re.IGNORECASE | re.DOTALL
    )
    
    if doc_match:
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠: ‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô HTML ‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏°‡∏≤‡πÉ‡∏ä‡πâ (‡∏ó‡∏¥‡πâ‡∏á‡∏Ç‡∏¢‡∏∞‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÑ‡∏õ‡πÄ‡∏•‡∏¢)
        html_content = doc_match.group(1)
    else:
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ pattern (‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå format ‡πÅ‡∏õ‡∏•‡∏Å): ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ß‡∏±‡∏á
        # ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏´‡∏≤ tag <TEXT> ‡πÅ‡∏£‡∏Å‡∏™‡∏∏‡∏î‡πÅ‡∏ó‡∏ô ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏´‡∏•‡∏±‡∏Å
        text_match = re.search(r'<TEXT>(.*?)</TEXT>', raw_content, re.IGNORECASE | re.DOTALL)
        if text_match:
            html_content = text_match.group(1)
        else:
            html_content = raw_content # ‡∏à‡∏ô‡∏õ‡∏±‡∏ç‡∏ç‡∏≤ ‡πÉ‡∏ä‡πâ‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°

    # --- Step 2: BeautifulSoup Cleaning (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°) ---
    soup = BeautifulSoup(html_content, "html.parser")
    
    # ‡∏•‡∏ö Tag ‡∏Ç‡∏¢‡∏∞ (Script, Style, ‡πÅ‡∏•‡∏∞ Table ‡∏ó‡∏µ‡πà‡∏ã‡πà‡∏≠‡∏ô‡πÑ‡∏ß‡πâ)
    for element in soup(["script", "style", "head", "meta", "link", "noscript"]):
        element.decompose()
        
    # (Optional) ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Base64/Binary ‡∏¢‡∏≤‡∏ß‡πÜ ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏´‡∏•‡∏∏‡∏î‡∏£‡∏≠‡∏î‡∏°‡∏≤
    # (‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡πâ‡∏≤‡∏°‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô tag graphic ‡πÅ‡∏ï‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô div)
    # ‡πÅ‡∏ï‡πà‡∏õ‡∏Å‡∏ï‡∏¥ Step 1 ‡∏à‡∏∞‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ 99% ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏£‡∏±‡∏ö

    # --- Step 3: Extract Text ---
    text = soup.get_text(separator=" ", strip=True)
    # ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏û‡∏ß‡∏Å us-gaap:AbcdefMember ‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ‡πÄ‡∏•‡∏¢
    text = re.sub(r'\b[a-z0-9]+:[A-Za-z0-9_]+Member\b', '', text)
    text = re.sub(r'\b[a-z0-9]+:[A-Za-z0-9_]+\b', '', text)
    
    # ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏´‡∏ô‡πâ‡∏≤‡∏õ‡∏Å‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç ---
    text = smart_crop_content(text)
    # ‡∏•‡∏ö Whitespace ‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô
    text = " ".join(text.split())
    
    return text

async def fetch_and_process_10k(user_id: int, ticker: str, amount: int = 1):
    ticker = ticker.upper()
    log.info(f"üîç Fetching 10-K for {ticker}...")

    company_dir = os.path.join(TEMP_SEC_DIR, "sec-edgar-filings", ticker)

    if os.path.exists(company_dir):
        log.info(f"üßπ Cleaning up old data for {ticker}...")
        shutil.rmtree(company_dir)

    dl = Downloader("Investi-Graph", settings.SEC_API_EMAIL, TEMP_SEC_DIR)

    try:
        dl.get("10-K", ticker, limit=amount)
        
        search_path = os.path.join(TEMP_SEC_DIR, "sec-edgar-filings", ticker, "10-K", "*", "*.txt")
        files = glob.glob(search_path)
        
        if not files:
            log.error(f"No 10-K found for {ticker}")
            return
        
        files.sort(reverse=True)

        file_path = files[0]
        log.info(f"üìÇ Found file: {file_path}")

        # 3. ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            raw_content = f.read()
            
        # --- 4. Clean HTML ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ---
        log.info("üßπ Cleaning HTML content...")
        clean_text = clean_html_content(raw_content)
        clean_text = smart_crop_content(clean_text)
        log.info(f"Cleaned text length: {len(clean_text)}")
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô bytes
        content_bytes = clean_text.encode("utf-8")
        filename = f"{ticker}_10K_Report.txt"

        # 5. ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡πÉ‡∏´‡πâ Pipeline (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
        async with SessionLocal() as db:
            db_doc = await crud.create_document(db=db, filename=filename, owner_id=user_id)
            
            await processing.save_extract_chunk_and_embed(
                document_id=db_doc.id,
                filename=filename,
                content_type="text/plain", # ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô Text ‡∏•‡πâ‡∏ß‡∏ô‡πÅ‡∏•‡πâ‡∏ß
                content=content_bytes
            )

        log.info(f"‚úÖ SEC Fetch & Process Complete for {ticker}")

    except Exception as e:
        log.error(f"‚ùå Error fetching SEC data: {e}")
    
    finally:
        if os.path.exists(os.path.join(TEMP_SEC_DIR, "sec-edgar-filings", ticker)):
             shutil.rmtree(os.path.join(TEMP_SEC_DIR, "sec-edgar-filings", ticker))