import asyncio
import logging
import os
import aiofiles
from pypdf import PdfReader
from sentence_transformers import SentenceTransformer, CrossEncoder
from langchain_text_splitters import RecursiveCharacterTextSplitter
from app import models, crud
from app.database import SessionLocal
from app.config import settings
import sqlalchemy as sa
from litellm import acompletion
from tenacity import retry, stop_after_attempt, wait_exponential, wait_fixed
from app import knowledge_graph
import re
from app.utils import smart_crop_content

UPLOAD_DIRECTORY = "/app/uploads"
log = logging.getLogger("uvicorn.error")

# --- 1. Load Models ---
log.info("Loading Embedding Model (Bi-Encoder)...")
EMBEDDING_MODEL = SentenceTransformer("all-MiniLM-L6-v2")

log.info("Loading Reranker Model (Cross-Encoder)...")
# ‡πÉ‡∏ä‡πâ‡∏£‡∏∏‡πà‡∏ô ms-marco-MiniLM-L-6-v2 (‡πÄ‡∏•‡πá‡∏Å ‡πÄ‡∏£‡πá‡∏ß ‡πÅ‡∏°‡πà‡∏ô)
RERANKER_MODEL = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2") 
log.info("Models loaded.")
# ----------------------

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
)

async def save_extract_chunk_and_embed(
    document_id: int,
    filename: str,
    content_type: str,
    content: bytes
):
    # ... (‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° 100% ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ) ...
    # (‡∏û‡∏µ‡πà‡∏Ç‡∏≠‡∏•‡∏∞‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡πâ‡∏ô‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡πÅ‡∏ï‡πà‡∏ô‡πâ‡∏≠‡∏á Copy ‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°‡∏°‡∏≤‡πÅ‡∏õ‡∏∞‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡∏à‡∏∞ Copy ‡∏ó‡∏±‡∏ö ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏û‡∏µ‡πà ‡πÄ‡∏î‡∏µ‡πã‡∏¢‡∏ß‡∏û‡∏µ‡πà‡πÅ‡∏õ‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏ï‡πá‡∏°‡πÉ‡∏´‡πâ)
    # ... (Logic ‡πÄ‡∏î‡∏¥‡∏°: Save File -> Extract -> Chunk -> Embed -> Save DB -> Graph Extract) ...
    os.makedirs(UPLOAD_DIRECTORY, exist_ok=True)
    file_path = os.path.join(UPLOAD_DIRECTORY, f"doc_{document_id}_{filename}")

    log.info(f"--- ü§ñ TASK START (Doc ID: {document_id}) ---")

    try:
        async with aiofiles.open(file_path, "wb") as out_file:
            await out_file.write(content)
        
        extracted_text = ""
        if content_type == "application/pdf":
            reader = PdfReader(file_path)
            for page in reader.pages:
                extracted_text += page.extract_text() + "\n"
            log.info("‚úÇÔ∏è Cropping PDF content...")
            extracted_text = smart_crop_content(extracted_text)
        else:
            extracted_text = content.decode("utf-8")

        chunks = text_splitter.split_text(extracted_text)
        
        # RAG Embed
        embeddings = EMBEDDING_MODEL.encode(chunks)
        db_chunks = []
        for i in range(len(chunks)):
            db_chunks.append(
                models.Chunk(text=chunks[i], embedding=embeddings[i], document_id=document_id)
            )

        async with SessionLocal() as db:
            db.add_all(db_chunks)
            await db.commit()
        
        # Graph Extract (Limit 5)
        MAX_GRAPH_CHUNKS = 5
        for i, chunk in enumerate(chunks):
            if i >= MAX_GRAPH_CHUNKS: break
            log.info(f"üß† Processing chunk {i+1}/{min(MAX_GRAPH_CHUNKS, len(chunks))} for graph extraction...")
            graph_data = await knowledge_graph.extract_graph_from_text(chunk)
            await knowledge_graph.store_graph_data(document_id, graph_data)
            # Small delay only for API courtesy (retries handle rate limits)
            if i < MAX_GRAPH_CHUNKS - 1:  # Don't sleep after the last chunk
                log.info("‚è≥ Sleeping 2s for API courtesy...")
                await asyncio.sleep(2)

        log.info(f"--- ü§ñ TASK DONE (Doc ID: {document_id}) ---")

    except Exception as e:
        log.error(f"Error processing: {e}")
    finally:
        if os.path.exists(file_path): os.remove(file_path)


# --- Reranking Helper Function ---
def rerank_chunks(query: str, chunks: list[models.Chunk], top_k: int = 5) -> list[models.Chunk]:
    """
    ‡∏£‡∏±‡∏ö Chunks ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å -> ‡πÉ‡∏ä‡πâ CrossEncoder ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö Query -> ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Top K
    """
    if not chunks:
        return []
    
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏π‡πà (Query, Document Text)
    pairs = [[query, chunk.text] for chunk in chunks]
    
    # ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (Scores)
    scores = RERANKER_MODEL.predict(pairs)
    
    # ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà Chunk ‡∏Å‡∏±‡∏ö Score
    chunk_score_pairs = list(zip(chunks, scores))
    
    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢
    sorted_pairs = sorted(chunk_score_pairs, key=lambda x: x[1], reverse=True)
    
    # ‡∏ï‡∏±‡∏î‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Top K
    top_chunks = [pair[0] for pair in sorted_pairs[:top_k]]
    
    log.info(f"Reranking done. Reduced {len(chunks)} -> {len(top_chunks)}")
    return top_chunks


# Retrieval (Global) - With Reranking
async def retrieve_relevant_chunks_global(user_id: int, query_text: str) -> list[models.Chunk]:
    log.info(f"Retrieving global (Stage 1: Vector Search)...")
    query_embedding = EMBEDDING_MODEL.encode(query_text)
    
    async with SessionLocal() as db:
        stmt = (
            sa.select(models.Chunk)
            .join(models.Document)
            .where(models.Document.owner_id == user_id)
            .order_by(models.Chunk.embedding.l2_distance(query_embedding))
            .limit(20) # <--- ‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÄ‡∏¢‡∏≠‡∏∞‡πÜ ‡∏Å‡πà‡∏≠‡∏ô (20)
        )
        result = await db.execute(stmt)
        initial_chunks = result.scalars().all()
        
    # Stage 2: Reranking
    return rerank_chunks(query_text, initial_chunks, top_k=5) # ‡∏Ñ‡∏±‡∏î‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 5


# Retrieval (Single Doc) - With Reranking
async def retrieve_relevant_chunks(document_id: int, query_text: str) -> list[models.Chunk]:
    log.info(f"Retrieving single doc (Stage 1: Vector Search)...")
    query_embedding = EMBEDDING_MODEL.encode(query_text)

    async with SessionLocal() as db:
        stmt = (
            sa.select(models.Chunk)
            .where(models.Chunk.document_id == document_id)
            .order_by(models.Chunk.embedding.l2_distance(query_embedding))
            .limit(20) # <--- ‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÄ‡∏¢‡∏≠‡∏∞‡πÜ ‡∏Å‡πà‡∏≠‡∏ô (20)
        )
        result = await db.execute(stmt)
        initial_chunks = result.scalars().all()

    # Stage 2: Reranking
    return rerank_chunks(query_text, initial_chunks, top_k=5) # ‡∏Ñ‡∏±‡∏î‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 5


# generate_answer
async def generate_answer(
    query: str, 
    context_chunks: list[models.Chunk],
    doc_id: int = None, # ‡∏£‡∏±‡∏ö doc_id ‡∏°‡∏≤‡∏î‡πâ‡∏ß‡∏¢ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    user_id: int = None # ‡∏´‡∏£‡∏∑‡∏≠ user_id (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö global)
) -> str:
    
    # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Vector Context (Text Chunks)
    vector_context = "\n\n".join([chunk.text for chunk in context_chunks])
    
    # 2. ‡∏´‡∏≤ Graph Context (‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô)
    log.info("Fetching GraphRAG context...")
    try:
        # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ doc_id ‡πÉ‡∏´‡πâ‡∏´‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô doc ‡∏ô‡∏±‡πâ‡∏ô, ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡∏´‡∏≤‡πÅ‡∏ö‡∏ö Global (‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ß‡∏±‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á Permission ‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï)
        # ‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ‡πÄ‡∏≠‡∏≤‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢‡∏Å‡πà‡∏≠‡∏ô ‡∏Ñ‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô Global Chat (doc_id=None) ‡πÄ‡∏£‡∏≤‡∏Ñ‡πâ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏•‡∏¢
        # ‡∏´‡∏£‡∏∑‡∏≠‡∏ô‡πâ‡∏≠‡∏á‡∏à‡∏∞‡∏™‡πà‡∏á user_id ‡πÑ‡∏õ‡∏Å‡∏£‡∏≠‡∏á‡πÉ‡∏ô Knowledge Graph ‡∏Å‡πá‡πÑ‡∏î‡πâ (Task Advance)
        graph_context = await knowledge_graph.query_graph_context(query, doc_id)
    except Exception as e:
        log.error(f"GraphRAG failed: {e}")
        graph_context = ""

    log.info(f"Generating answer using {len(context_chunks)} chunks + Graph Context.")

    # 3. ‡∏£‡∏ß‡∏° Prompt
    prompt = f"""
    You are an expert financial analyst AI.
    Answer the user's question based on the context provided below.
    
    The context consists of:
    1. "Document Excerpts": Text retrieved from the document files.
    2. "Knowledge Graph": Relationships extracted from the data.

    Combine both sources to give a comprehensive answer.
    If the answer is not found, say so.

    --- DOCUMENT EXCERPTS ---
    {vector_context}
    
    --- KNOWLEDGE GRAPH ---
    {graph_context}
    ---

    QUESTION:
    {query}
    """

    @retry(stop=stop_after_attempt(3), wait=wait_fixed(2))
    async def call_llm_api():
        return await acompletion(
            model=f"{settings.LLM_PROVIDER}/llama-3.1-8b-instant",
            api_key=settings.LLM_API_KEY,
            messages=[
                {"role": "system", "content": "You are a helpful analyst."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.0
        )

    try:
        response = await call_llm_api()
        return response.choices[0].message.content
    except Exception as e:
        log.error(f"Generation failed: {e}")
        return "Error generating response."
    